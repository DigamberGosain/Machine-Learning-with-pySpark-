{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ab1ec53",
   "metadata": {},
   "source": [
    "# <span style = 'color:#960574;font-family:helvetica'> NLP basiscs with pySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f5e5634",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "154b0b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "findspark.init('/home/chandan/spark-3.2.4-bin-hadoop3.2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab74e56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65a799e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/21 15:57:54 WARN Utils: Your hostname, chandan-VivoBook-ASUSLaptop-X515MA-X515MA resolves to a loopback address: 127.0.1.1; using 192.168.0.169 instead (on interface wlo1)\n",
      "23/05/21 15:57:54 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/chandan/spark-3.2.4-bin-hadoop3.2/jars/spark-unsafe_2.12-3.2.4.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/05/21 15:57:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('nlp').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65fe7773",
   "metadata": {},
   "source": [
    "## pySpark tools for Text data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2b9e36",
   "metadata": {},
   "source": [
    "### Spark comes with Tokenziser class and also Regular Expression class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "147a17cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f78eea47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9505d4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "sen_df = spark.createDataFrame([\n",
    "    (0,'Hi I heard about Spark'),\n",
    "    (1,\"I wish Java could use case classes\"),\n",
    "    (2,\"Logistic,regression,models,are,neat\")\n",
    "    \n",
    "],['id','sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "146a4890",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "| id|            sentence|\n",
      "+---+--------------------+\n",
      "|  0|Hi I heard about ...|\n",
      "|  1|I wish Java could...|\n",
      "|  2|Logistic,regressi...|\n",
      "+---+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sen_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3891008e",
   "metadata": {},
   "source": [
    "### We will used tokernizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "023fbf4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol='sentence', outputCol='words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "053da170",
   "metadata": {},
   "outputs": [],
   "source": [
    "regex_tokenziser = RegexTokenizer(inputCol='sentence', outputCol='words', pattern='\\\\W')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a89518ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_tokens = udf(lambda words: len(words), IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "da3df49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = tokenizer.transform(sen_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "440c823c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 2:>                                                          (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+\n",
      "| id|            sentence|               words|\n",
      "+---+--------------------+--------------------+\n",
      "|  0|Hi I heard about ...|[hi, i, heard, ab...|\n",
      "|  1|I wish Java could...|[i, wish, java, c...|\n",
      "|  2|Logistic,regressi...|[logistic,regress...|\n",
      "+---+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenized.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eb7161f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+------+\n",
      "| id|            sentence|               words|tokens|\n",
      "+---+--------------------+--------------------+------+\n",
      "|  0|Hi I heard about ...|[hi, i, heard, ab...|     5|\n",
      "|  1|I wish Java could...|[i, wish, java, c...|     7|\n",
      "|  2|Logistic,regressi...|[logistic,regress...|     1|\n",
      "+---+--------------------+--------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenized.withColumn('tokens', count_tokens(col('words'))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118a20cf",
   "metadata": {},
   "source": [
    "### We can see tokenizer splits on white spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "872d3159",
   "metadata": {},
   "outputs": [],
   "source": [
    "rg_tokenized = regex_tokenziser.transform(sen_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ac95db2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+------+\n",
      "| id|            sentence|               words|tokens|\n",
      "+---+--------------------+--------------------+------+\n",
      "|  0|Hi I heard about ...|[hi, i, heard, ab...|     5|\n",
      "|  1|I wish Java could...|[i, wish, java, c...|     7|\n",
      "|  2|Logistic,regressi...|[logistic, regres...|     5|\n",
      "+---+--------------------+--------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rg_tokenized.withColumn('tokens', count_tokens(col('words'))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e035004",
   "metadata": {},
   "source": [
    "#### With regex_tokenizer we were able to seperate even on comma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c8f00d",
   "metadata": {},
   "source": [
    "### Stop words removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "11073ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2debbe35",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentenceDataFrame = spark.createDataFrame([\n",
    "    (0,['I',\"saw\",\"the\",'green','horse']),\n",
    "    (1,[\"Mary\", 'had', 'a', 'little', 'lamb'])\n",
    "],['id','tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3b427df9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "| id|              tokens|\n",
      "+---+--------------------+\n",
      "|  0|[I, saw, the, gre...|\n",
      "|  1|[Mary, had, a, li...|\n",
      "+---+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentenceDataFrame.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7c052d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "remover = StopWordsRemover(inputCol='tokens',outputCol= 'filtered')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8e64afec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+\n",
      "| id|              tokens|            filtered|\n",
      "+---+--------------------+--------------------+\n",
      "|  0|[I, saw, the, gre...| [saw, green, horse]|\n",
      "|  1|[Mary, had, a, li...|[Mary, little, lamb]|\n",
      "+---+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "remover.transform(sentenceDataFrame).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5d4a90",
   "metadata": {},
   "source": [
    "## n-gram  is sequence of tokens typically words of an integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5e04826c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import NGram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7fc5f330",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordDataFrame = spark.createDataFrame([\n",
    "    (0,['Hi',\"I\",\"heard\",\"about\",\"Spark\"]),\n",
    "    (1,[\"I\",\"wish\",\"Java\",'could', 'use','case','classes']),\n",
    "    (2,['Logistic',\"regression\",\"models\", \"are\",'neat'])\n",
    "], [\"id\",'words'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e57c437d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "| id|               words|\n",
      "+---+--------------------+\n",
      "|  0|[Hi, I, heard, ab...|\n",
      "|  1|[I, wish, Java, c...|\n",
      "|  2|[Logistic, regres...|\n",
      "+---+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wordDataFrame.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fa978d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram =NGram(n=2,inputCol='words', outputCol='grams')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cfa82153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------+\n",
      "|grams                                                             |\n",
      "+------------------------------------------------------------------+\n",
      "|[Hi I, I heard, heard about, about Spark]                         |\n",
      "|[I wish, wish Java, Java could, could use, use case, case classes]|\n",
      "|[Logistic regression, regression models, models are, are neat]    |\n",
      "+------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ngram.transform(wordDataFrame).select('grams').show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196207d5",
   "metadata": {},
   "source": [
    "### n-grams are creating pairs of consicutive words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477b4c6f",
   "metadata": {},
   "source": [
    "It helps to build on word relationship"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9606b6",
   "metadata": {},
   "source": [
    "### TF-IDF and Count Vecotrizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a071f908",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF,IDF, Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ac2c26bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentenceData = spark.createDataFrame([\n",
    "    (0,'Hi I heard about Spark'),\n",
    "    (1,\"I wish Java could use case classes\"),\n",
    "    (2,\"Logistic,regression,models,are,neat\")\n",
    "    \n",
    "],['id','sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1f93dae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------------------------+\n",
      "|id |sentence                           |\n",
      "+---+-----------------------------------+\n",
      "|0  |Hi I heard about Spark             |\n",
      "|1  |I wish Java could use case classes |\n",
      "|2  |Logistic,regression,models,are,neat|\n",
      "+---+-----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentenceData.show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ef6f1730",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol='sentence', outputCol='words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ed3798df",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_data = tokenizer.transform(sentenceData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a317f456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+\n",
      "| id|            sentence|               words|\n",
      "+---+--------------------+--------------------+\n",
      "|  0|Hi I heard about ...|[hi, i, heard, ab...|\n",
      "|  1|I wish Java could...|[i, wish, java, c...|\n",
      "|  2|Logistic,regressi...|[logistic,regress...|\n",
      "+---+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "words_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95ce685",
   "metadata": {},
   "source": [
    "## Term frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0d8b3ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "hashing_tf = HashingTF(inputCol='words', outputCol='rawFeatures')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9fd33191",
   "metadata": {},
   "outputs": [],
   "source": [
    "featurized_data = hashing_tf.transform(words_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75322332",
   "metadata": {},
   "source": [
    "### Now can appy Inverse document factorizer on hashed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1a85148e",
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = IDF(inputCol='rawFeatures', outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2f012a5c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "idf_model = idf.fit(featurized_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "67d40ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "rescaled_data = idf_model.transform(featurized_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e130fae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/21 16:39:38 WARN DAGScheduler: Broadcasting large task binary with size 4.0 MiB\n",
      "23/05/21 16:39:38 WARN DAGScheduler: Broadcasting large task binary with size 4.0 MiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+--------------------+--------------------+\n",
      "| id|            sentence|               words|         rawFeatures|            features|\n",
      "+---+--------------------+--------------------+--------------------+--------------------+\n",
      "|  0|Hi I heard about ...|[hi, i, heard, ab...|(262144,[18700,19...|(262144,[18700,19...|\n",
      "|  1|I wish Java could...|[i, wish, java, c...|(262144,[19036,20...|(262144,[19036,20...|\n",
      "|  2|Logistic,regressi...|[logistic,regress...|(262144,[11534],[...|(262144,[11534],[...|\n",
      "+---+--------------------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rescaled_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f99119",
   "metadata": {},
   "source": [
    "## Countvectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ca153c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "36a79a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame([\n",
    "    (0,'a b c'.split(\" \")),\n",
    "    (1,'a b b c a'.split(\" \"))\n",
    "], ['id','words'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a5711790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+\n",
      "| id|          words|\n",
      "+---+---------------+\n",
      "|  0|      [a, b, c]|\n",
      "|  1|[a, b, b, c, a]|\n",
      "+---+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "59114da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(inputCol='words', outputCol='features', vocabSize=3, minDF=2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "bc8eb4c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "model = cv.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d79243ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7e3d0877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+-------------------------+\n",
      "|id |words          |features                 |\n",
      "+---+---------------+-------------------------+\n",
      "|0  |[a, b, c]      |(3,[0,1,2],[1.0,1.0,1.0])|\n",
      "|1  |[a, b, b, c, a]|(3,[0,1,2],[2.0,2.0,1.0])|\n",
      "+---+---------------+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results.show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0acefd6c",
   "metadata": {},
   "source": [
    "# Practical Excerise Spam Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2af9d528",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data = spark.read.csv(\"SMSSpamCollection\", inferSchema=True, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "06811190",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.withColumnRenamed('_c0','class').withColumnRenamed('_c1','text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2e15c7fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|class|                text|\n",
      "+-----+--------------------+\n",
      "|  ham|Go until jurong p...|\n",
      "|  ham|Ok lar... Joking ...|\n",
      "| spam|Free entry in 2 a...|\n",
      "|  ham|U dun say so earl...|\n",
      "|  ham|Nah I don't think...|\n",
      "| spam|FreeMsg Hey there...|\n",
      "|  ham|Even my brother i...|\n",
      "|  ham|As per your reque...|\n",
      "| spam|WINNER!! As a val...|\n",
      "| spam|Had your mobile 1...|\n",
      "|  ham|I'm gonna be home...|\n",
      "| spam|SIX chances to wi...|\n",
      "| spam|URGENT! You have ...|\n",
      "|  ham|I've been searchi...|\n",
      "|  ham|I HAVE A DATE ON ...|\n",
      "| spam|XXXMobileMovieClu...|\n",
      "|  ham|Oh k...i'm watchi...|\n",
      "|  ham|Eh u remember how...|\n",
      "|  ham|Fine if thats th...|\n",
      "| spam|England v Macedon...|\n",
      "+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b4ab4ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b66ba7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.withColumn('length', length(data['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "810cc0b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+------+\n",
      "|class|                text|length|\n",
      "+-----+--------------------+------+\n",
      "|  ham|Go until jurong p...|   111|\n",
      "|  ham|Ok lar... Joking ...|    29|\n",
      "| spam|Free entry in 2 a...|   155|\n",
      "|  ham|U dun say so earl...|    49|\n",
      "|  ham|Nah I don't think...|    61|\n",
      "| spam|FreeMsg Hey there...|   147|\n",
      "|  ham|Even my brother i...|    77|\n",
      "|  ham|As per your reque...|   160|\n",
      "| spam|WINNER!! As a val...|   157|\n",
      "| spam|Had your mobile 1...|   154|\n",
      "|  ham|I'm gonna be home...|   109|\n",
      "| spam|SIX chances to wi...|   136|\n",
      "| spam|URGENT! You have ...|   155|\n",
      "|  ham|I've been searchi...|   196|\n",
      "|  ham|I HAVE A DATE ON ...|    35|\n",
      "| spam|XXXMobileMovieClu...|   149|\n",
      "|  ham|Oh k...i'm watchi...|    26|\n",
      "|  ham|Eh u remember how...|    81|\n",
      "|  ham|Fine if thats th...|    56|\n",
      "| spam|England v Macedon...|   155|\n",
      "+-----+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "129d36e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 44:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------------+\n",
      "|class|      avg(length)|\n",
      "+-----+-----------------+\n",
      "|  ham|71.45431945307645|\n",
      "| spam|138.6706827309237|\n",
      "+-----+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data.groupBy('class').mean().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1e19d2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover,CountVectorizer,IDF, StringIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "295ebba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol='text', outputCol='token_text')\n",
    "stop_remove = StopWordsRemover(inputCol='token_text', outputCol='stop_token')\n",
    "count_vec = CountVectorizer(inputCol='stop_token', outputCol='c_vec')\n",
    "idf = IDF(inputCol='c_vec', outputCol='tf_idf')\n",
    "ham_spam_to_nummeric = StringIndexer(inputCol='class', outputCol='label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "71974e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "6d33554c",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_up = VectorAssembler(inputCols=['tf_idf','length'], outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "77e610d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import NaiveBayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e1b4c6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = NaiveBayes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "06685219",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e0970580",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_prep_pipe = Pipeline(stages=[\n",
    "    ham_spam_to_nummeric, tokenizer, stop_remove, count_vec, idf, clean_up\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "dbfa664c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "cleaner = data_prep_pipe.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "6c3ea13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data = cleaner.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "1e6f044a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data = cleaned_data.select('label','features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "d99b8852",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 55:>                                                         (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|  0.0|(13424,[7,11,31,6...|\n",
      "|  0.0|(13424,[0,24,297,...|\n",
      "|  1.0|(13424,[2,13,19,3...|\n",
      "|  0.0|(13424,[0,70,80,1...|\n",
      "|  0.0|(13424,[36,134,31...|\n",
      "|  1.0|(13424,[10,60,139...|\n",
      "|  0.0|(13424,[10,53,103...|\n",
      "|  0.0|(13424,[125,184,4...|\n",
      "|  1.0|(13424,[1,47,118,...|\n",
      "|  1.0|(13424,[0,1,13,27...|\n",
      "|  0.0|(13424,[18,43,120...|\n",
      "|  1.0|(13424,[8,17,37,8...|\n",
      "|  1.0|(13424,[13,30,47,...|\n",
      "|  0.0|(13424,[39,96,217...|\n",
      "|  0.0|(13424,[552,1697,...|\n",
      "|  1.0|(13424,[30,109,11...|\n",
      "|  0.0|(13424,[82,214,47...|\n",
      "|  0.0|(13424,[0,2,49,13...|\n",
      "|  0.0|(13424,[0,74,105,...|\n",
      "|  1.0|(13424,[4,30,33,5...|\n",
      "+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cleaned_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "5a1523eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "training, test = cleaned_data.randomSplit([0.7,0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "6380cafb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/21 17:02:41 WARN DAGScheduler: Broadcasting large task binary with size 1142.7 KiB\n",
      "23/05/21 17:02:47 WARN DAGScheduler: Broadcasting large task binary with size 1126.3 KiB\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spam_detector = nb.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "be48871a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = spam_detector.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a4c22a52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/21 17:03:28 WARN DAGScheduler: Broadcasting large task binary with size 1360.1 KiB\n",
      "[Stage 59:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|label|            features|       rawPrediction|         probability|prediction|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|  0.0|(13424,[0,1,2,41,...|[-1062.2072173855...|[1.0,3.8985651638...|       0.0|\n",
      "|  0.0|(13424,[0,1,5,15,...|[-998.36436082150...|[1.0,3.9496523954...|       0.0|\n",
      "|  0.0|(13424,[0,1,7,8,1...|[-870.06851304074...|[1.0,4.8982570410...|       0.0|\n",
      "|  0.0|(13424,[0,1,9,14,...|[-549.11720978981...|[1.0,8.0444295900...|       0.0|\n",
      "|  0.0|(13424,[0,1,14,78...|[-692.52358394833...|[1.0,8.1051838478...|       0.0|\n",
      "|  0.0|(13424,[0,1,15,20...|[-693.10138322613...|[1.0,2.0246190266...|       0.0|\n",
      "|  0.0|(13424,[0,1,17,19...|[-804.92516203141...|[1.0,6.6609501986...|       0.0|\n",
      "|  0.0|(13424,[0,1,20,27...|[-966.66976540239...|[1.0,3.4240278092...|       0.0|\n",
      "|  0.0|(13424,[0,1,27,35...|[-1471.3876569655...|[1.0,1.3544380420...|       0.0|\n",
      "|  0.0|(13424,[0,1,27,88...|[-1552.5583514182...|[0.99683551001422...|       0.0|\n",
      "|  0.0|(13424,[0,1,72,10...|[-666.51912218566...|[1.0,5.6314218659...|       0.0|\n",
      "|  0.0|(13424,[0,1,146,1...|[-254.18200679757...|[0.04131441179532...|       1.0|\n",
      "|  0.0|(13424,[0,1,874,1...|[-97.364960825663...|[0.99999989632465...|       0.0|\n",
      "|  0.0|(13424,[0,1,874,1...|[-99.051903508384...|[0.99999992131442...|       0.0|\n",
      "|  0.0|(13424,[0,2,3,4,6...|[-1296.7006601555...|[1.0,1.3512767916...|       0.0|\n",
      "|  0.0|(13424,[0,2,3,5,6...|[-2551.0859011823...|[1.0,6.2030679993...|       0.0|\n",
      "|  0.0|(13424,[0,2,3,5,3...|[-506.23894177785...|[0.99999999999999...|       0.0|\n",
      "|  0.0|(13424,[0,2,3,6,9...|[-3279.8225445465...|[1.0,1.2609979376...|       0.0|\n",
      "|  0.0|(13424,[0,2,3,8,2...|[-1613.3040285895...|[1.0,1.2793418677...|       0.0|\n",
      "|  0.0|(13424,[0,2,4,7,2...|[-516.62969985950...|[1.0,2.3437389916...|       0.0|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "test_results.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "2e063c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "e9a06ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_eval =MulticlassClassificationEvaluator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "9a084c94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/21 17:04:31 WARN DAGScheduler: Broadcasting large task binary with size 1365.2 KiB\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "acc = acc_eval.evaluate(test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "0063fde2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9300343993352429\n"
     ]
    }
   ],
   "source": [
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a42205",
   "metadata": {},
   "source": [
    "Thank you !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbdc892",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
