{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00f23685",
   "metadata": {},
   "source": [
    "# <span style = 'color:green;font-family:helvetica'> Logistic Regression with pySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce8499d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49d330d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "findspark.init('/home/chandan/spark-3.2.4-bin-hadoop3.2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357ce607",
   "metadata": {},
   "source": [
    "#### pySpark Documentation Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26e63d3",
   "metadata": {},
   "source": [
    "* With Logistic Regression we will also get introduced to the concept of `\"Evaluators\"`.\n",
    "\n",
    "* Evaluators behave simialr to Machine Learning algorithm object, but are designed to take in evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52ff970a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae36b484",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f3ff789",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/20 14:15:19 WARN Utils: Your hostname, chandan-VivoBook-ASUSLaptop-X515MA-X515MA resolves to a loopback address: 127.0.1.1; using 192.168.0.169 instead (on interface wlo1)\n",
      "23/05/20 14:15:19 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/chandan/spark-3.2.4-bin-hadoop3.2/jars/spark-unsafe_2.12-3.2.4.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/05/20 14:15:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"mylogreg\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9c88bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8bd43c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/20 14:15:26 WARN LibSVMFileFormat: 'numFeatures' option not specified, determining the number of features by going though the input. If you know the number in advance, please specify it via 'numFeatures' option to avoid the extra scan.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "my_data = spark.read.format('libsvm').load(\"sample_libsvm_data.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "033f7521",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|  0.0|(692,[127,128,129...|\n",
      "|  1.0|(692,[158,159,160...|\n",
      "|  1.0|(692,[124,125,126...|\n",
      "|  1.0|(692,[152,153,154...|\n",
      "|  1.0|(692,[151,152,153...|\n",
      "|  0.0|(692,[129,130,131...|\n",
      "|  1.0|(692,[158,159,160...|\n",
      "|  1.0|(692,[99,100,101,...|\n",
      "|  0.0|(692,[154,155,156...|\n",
      "|  0.0|(692,[127,128,129...|\n",
      "|  1.0|(692,[154,155,156...|\n",
      "|  0.0|(692,[153,154,155...|\n",
      "|  0.0|(692,[151,152,153...|\n",
      "|  1.0|(692,[129,130,131...|\n",
      "|  0.0|(692,[154,155,156...|\n",
      "|  1.0|(692,[150,151,152...|\n",
      "|  0.0|(692,[124,125,126...|\n",
      "|  0.0|(692,[152,153,154...|\n",
      "|  1.0|(692,[97,98,99,12...|\n",
      "|  1.0|(692,[124,125,126...|\n",
      "+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "my_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e31d632d",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_log_reg = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "878a9765",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/20 14:15:41 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "23/05/20 14:15:41 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.ForeignLinkerBLAS\n",
      "23/05/20 14:15:41 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "23/05/20 14:15:41 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n"
     ]
    }
   ],
   "source": [
    "log_reg_model = my_log_reg.fit(my_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a1b11414",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_summary = log_reg_model.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b8d89ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chandan/spark-3.2.4-bin-hadoop3.2/python/pyspark/sql/context.py:125: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[label: double, features: vector, rawPrediction: vector, probability: vector, prediction: double]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_summary.predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bec82e96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|label|            features|       rawPrediction|         probability|prediction|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|  0.0|(692,[127,128,129...|[20.3777627514872...|[0.99999999858729...|       0.0|\n",
      "|  1.0|(692,[158,159,160...|[-21.114014198868...|[6.76550380000472...|       1.0|\n",
      "|  1.0|(692,[124,125,126...|[-23.743613234676...|[4.87842678716177...|       1.0|\n",
      "|  1.0|(692,[152,153,154...|[-19.192574012720...|[4.62137287298144...|       1.0|\n",
      "|  1.0|(692,[151,152,153...|[-20.125398874699...|[1.81823629113068...|       1.0|\n",
      "|  0.0|(692,[129,130,131...|[20.4890549504196...|[0.99999999873608...|       0.0|\n",
      "|  1.0|(692,[158,159,160...|[-21.082940212814...|[6.97903542823766...|       1.0|\n",
      "|  1.0|(692,[99,100,101,...|[-19.622713503550...|[3.00582577446132...|       1.0|\n",
      "|  0.0|(692,[154,155,156...|[21.1594863606582...|[0.99999999935352...|       0.0|\n",
      "|  0.0|(692,[127,128,129...|[28.1036706837287...|[0.99999999999937...|       0.0|\n",
      "|  1.0|(692,[154,155,156...|[-21.054076780106...|[7.18340962960324...|       1.0|\n",
      "|  0.0|(692,[153,154,155...|[26.9648490510184...|[0.99999999999805...|       0.0|\n",
      "|  0.0|(692,[151,152,153...|[32.7855654161400...|[0.99999999999999...|       0.0|\n",
      "|  1.0|(692,[129,130,131...|[-20.331839179667...|[1.47908944089721...|       1.0|\n",
      "|  0.0|(692,[154,155,156...|[21.7830579106564...|[0.99999999965347...|       0.0|\n",
      "|  1.0|(692,[150,151,152...|[-20.640562103728...|[1.08621994880353...|       1.0|\n",
      "|  0.0|(692,[124,125,126...|[22.6400775503731...|[0.99999999985292...|       0.0|\n",
      "|  0.0|(692,[152,153,154...|[38.0712919910909...|           [1.0,0.0]|       0.0|\n",
      "|  1.0|(692,[97,98,99,12...|[-19.830803265627...|[2.44113371545874...|       1.0|\n",
      "|  1.0|(692,[124,125,126...|[-21.016054806036...|[7.46179590484091...|       1.0|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_summary.predictions.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a53523e",
   "metadata": {},
   "source": [
    "#### As we saw the general work around of Logistic Regression, now we will se the concept of Evaluators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8cab0858",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_train, lr_test = my_data.randomSplit([0.7,0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "596c3bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "00969213",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_final = final_model.fit(lr_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "67386e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_and_labels = fit_final.evaluate(lr_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f19f06e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chandan/spark-3.2.4-bin-hadoop3.2/python/pyspark/sql/context.py:125: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|label|            features|       rawPrediction|         probability|prediction|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|  0.0|(692,[95,96,97,12...|[20.1778353375048...|[0.99999999827464...|       0.0|\n",
      "|  0.0|(692,[100,101,102...|[1.19708033509664...|[0.76800498418951...|       0.0|\n",
      "|  0.0|(692,[124,125,126...|[16.6124397300044...|[0.99999993900291...|       0.0|\n",
      "|  0.0|(692,[126,127,128...|[26.5120701373837...|[0.99999999999693...|       0.0|\n",
      "|  0.0|(692,[127,128,129...|[19.1626470695618...|[0.99999999523823...|       0.0|\n",
      "|  0.0|(692,[128,129,130...|[19.8634824897547...|[0.99999999763735...|       0.0|\n",
      "|  0.0|(692,[152,153,154...|[10.3945175147437...|[0.99996940114946...|       0.0|\n",
      "|  0.0|(692,[234,235,237...|[-2.1300789874252...|[0.10620749336616...|       1.0|\n",
      "|  1.0|(692,[97,98,99,12...|[-14.215142998947...|[6.70566023838883...|       1.0|\n",
      "|  1.0|(692,[119,120,121...|[-0.8450178292576...|[0.30047902476796...|       1.0|\n",
      "|  1.0|(692,[123,124,125...|[-18.762869650743...|[7.10214022739320...|       1.0|\n",
      "|  1.0|(692,[124,125,126...|[-16.437089496666...|[7.26879969889195...|       1.0|\n",
      "|  1.0|(692,[126,127,128...|[-28.494747220850...|[4.21588269915733...|       1.0|\n",
      "|  1.0|(692,[126,127,128...|[-21.921106984720...|[3.01845148000591...|       1.0|\n",
      "|  1.0|(692,[127,128,129...|[-28.944293371468...|[2.68938579206040...|       1.0|\n",
      "|  1.0|(692,[127,128,155...|[-23.804680013927...|[4.58943080593228...|       1.0|\n",
      "|  1.0|(692,[128,129,130...|[-22.521971246477...|[1.65513021136661...|       1.0|\n",
      "|  1.0|(692,[128,129,130...|[-18.270912397000...|[1.16156511796417...|       1.0|\n",
      "|  1.0|(692,[129,130,131...|[-24.421162898984...|[2.47755732184565...|       1.0|\n",
      "|  1.0|(692,[129,130,131...|[-17.569445901003...|[2.34253767799463...|       1.0|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction_and_labels.predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fe321d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5cd3b97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_eval = BinaryClassificationEvaluator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9f77fd5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chandan/spark-3.2.4-bin-hadoop3.2/python/pyspark/sql/context.py:125: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "final_roc_results = my_eval.evaluate(prediction_and_labels.predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8b71a0b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9943181818181819"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_roc_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e599dc55",
   "metadata": {},
   "source": [
    "#### This means the ROC that almost 100%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b368fc",
   "metadata": {},
   "source": [
    "#### Another Example with Titanic dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "46839af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"titanic.csv\", inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "16fc4979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PassengerId: integer (nullable = true)\n",
      " |-- Survived: integer (nullable = true)\n",
      " |-- Pclass: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- SibSp: integer (nullable = true)\n",
      " |-- Parch: integer (nullable = true)\n",
      " |-- Ticket: string (nullable = true)\n",
      " |-- Fare: double (nullable = true)\n",
      " |-- Cabin: string (nullable = true)\n",
      " |-- Embarked: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "44616692",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_cols = df.select(['Survived','Pclass','Sex',\n",
    "                     'Age','SibSp','Parch','Fare','Embarked'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8643e45e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+------+----+-----+-----+-------+--------+\n",
      "|Survived|Pclass|   Sex| Age|SibSp|Parch|   Fare|Embarked|\n",
      "+--------+------+------+----+-----+-----+-------+--------+\n",
      "|       0|     3|  male|22.0|    1|    0|   7.25|       S|\n",
      "|       1|     1|female|38.0|    1|    0|71.2833|       C|\n",
      "|       1|     3|female|26.0|    0|    0|  7.925|       S|\n",
      "|       1|     1|female|35.0|    1|    0|   53.1|       S|\n",
      "|       0|     3|  male|35.0|    0|    0|   8.05|       S|\n",
      "|       0|     3|  male|null|    0|    0| 8.4583|       Q|\n",
      "|       0|     1|  male|54.0|    0|    0|51.8625|       S|\n",
      "|       0|     3|  male| 2.0|    3|    1| 21.075|       S|\n",
      "|       1|     3|female|27.0|    0|    2|11.1333|       S|\n",
      "|       1|     2|female|14.0|    1|    0|30.0708|       C|\n",
      "|       1|     3|female| 4.0|    1|    1|   16.7|       S|\n",
      "|       1|     1|female|58.0|    0|    0|  26.55|       S|\n",
      "|       0|     3|  male|20.0|    0|    0|   8.05|       S|\n",
      "|       0|     3|  male|39.0|    1|    5| 31.275|       S|\n",
      "|       0|     3|female|14.0|    0|    0| 7.8542|       S|\n",
      "|       1|     2|female|55.0|    0|    0|   16.0|       S|\n",
      "|       0|     3|  male| 2.0|    4|    1| 29.125|       Q|\n",
      "|       1|     2|  male|null|    0|    0|   13.0|       S|\n",
      "|       0|     3|female|31.0|    1|    0|   18.0|       S|\n",
      "|       1|     3|female|null|    0|    0|  7.225|       C|\n",
      "+--------+------+------+----+-----+-----+-------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "my_cols.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7e8883",
   "metadata": {},
   "source": [
    "#### Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "51840b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_final_data = my_cols.na.drop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5637851",
   "metadata": {},
   "source": [
    " #### Working with categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ff7ecaeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler,VectorIndexer,OneHotEncoder,StringIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "87a67894",
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_indexer = StringIndexer(inputCol='Sex', outputCol='SexIndex')\n",
    "gender_encoder = OneHotEncoder(inputCol= \"SexIndex\", outputCol='SexVec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "61c8ee9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "embark_indexer = StringIndexer(inputCol='Embarked', outputCol='EmbarkIndex')\n",
    "embark_encoder = OneHotEncoder(inputCol= 'EmbarkIndex', outputCol='EmbarkVec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ff437eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=['Pclass','SexVec','EmbarkVec',\"Age\",\"SibSp\",\"Parch\",'Fare'],\n",
    "                           outputCol= 'features')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70545408",
   "metadata": {},
   "source": [
    "### Creating a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fa17f5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5c0e0b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7313e9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_titanic = LogisticRegression(featuresCol='features', labelCol='Survived')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "47006c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[gender_indexer,embark_indexer,\n",
    "                           gender_encoder,embark_encoder,\n",
    "                           assembler, log_reg_titanic])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b359a7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = my_final_data.randomSplit([0.7,0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "25c2bacb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "fit_model =  pipeline.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "80b6b5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = fit_model.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d54c3e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "83935d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_eval = BinaryClassificationEvaluator(rawPredictionCol= 'prediction',labelCol='Survived')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3fa0892d",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUC = my_eval.evaluate(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "98ef6102",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8207288042483062"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AUC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f077610",
   "metadata": {},
   "source": [
    "The Area under curve close to 1 which means model is good"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a5f196",
   "metadata": {},
   "source": [
    "# Practice Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a4f95757",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('customer_churn.csv', inferSchema=True, header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9f52796e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(Names='Cameron Williams', Age=42.0, Total_Purchase=11066.8, Account_Manager=0, Years=7.22, Num_Sites=8.0, Onboard_date='2013-08-30 07:00:40', Location='10265 Elizabeth Mission Barkerburgh, AK 89518', Company='Harvey LLC', Churn=1)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "dc8afdf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Names: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- Total_Purchase: double (nullable = true)\n",
      " |-- Account_Manager: integer (nullable = true)\n",
      " |-- Years: double (nullable = true)\n",
      " |-- Num_Sites: double (nullable = true)\n",
      " |-- Onboard_date: string (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- Company: string (nullable = true)\n",
      " |-- Churn: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a8e9bb2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 113:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------+-----------------+-----------------+------------------+-----------------+------------------+-------------------+--------------------+--------------------+-------------------+\n",
      "|summary|        Names|              Age|   Total_Purchase|   Account_Manager|            Years|         Num_Sites|       Onboard_date|            Location|             Company|              Churn|\n",
      "+-------+-------------+-----------------+-----------------+------------------+-----------------+------------------+-------------------+--------------------+--------------------+-------------------+\n",
      "|  count|          900|              900|              900|               900|              900|               900|                900|                 900|                 900|                900|\n",
      "|   mean|         null|41.81666666666667|10062.82403333334|0.4811111111111111| 5.27315555555555| 8.587777777777777|               null|                null|                null|0.16666666666666666|\n",
      "| stddev|         null|6.127560416916251|2408.644531858096|0.4999208935073339|1.274449013194616|1.7648355920350969|               null|                null|                null| 0.3728852122772358|\n",
      "|    min|   Aaron King|             22.0|            100.0|                 0|              1.0|               3.0|2006-01-02 04:16:13|00103 Jeffrey Cre...|     Abbott-Thompson|                  0|\n",
      "|    max|Zachary Walsh|             65.0|         18026.01|                 1|             9.15|              14.0|2016-12-28 04:07:38|Unit 9800 Box 287...|Zuniga, Clark and...|                  1|\n",
      "+-------+-------------+-----------------+-----------------+------------------+-----------------+------------------+-------------------+--------------------+--------------------+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b96adb38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Names',\n",
       " 'Age',\n",
       " 'Total_Purchase',\n",
       " 'Account_Manager',\n",
       " 'Years',\n",
       " 'Num_Sites',\n",
       " 'Onboard_date',\n",
       " 'Location',\n",
       " 'Company',\n",
       " 'Churn']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6ce92f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9cbc9572",
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=['Age',\n",
    " 'Total_Purchase',\n",
    " 'Account_Manager',\n",
    " 'Years',\n",
    " 'Num_Sites',], outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "66dbf133",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = assembler.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d4a03f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = output.select(['features','churn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "09c65f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_churn, test_churn = final_data.randomSplit([0.7,0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "08752071",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b9acafb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_churn = LogisticRegression(labelCol='churn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "74055765",
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_churn_model = lr_churn.fit(train_churn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "acc04cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_sum = fitted_churn_model.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "db4e4b9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chandan/spark-3.2.4-bin-hadoop3.2/python/pyspark/sql/context.py:125: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+-------------------+\n",
      "|summary|              churn|         prediction|\n",
      "+-------+-------------------+-------------------+\n",
      "|  count|                617|                617|\n",
      "|   mean|0.15883306320907617| 0.1166936790923825|\n",
      "| stddev|0.36581691140689965|0.32131541505231426|\n",
      "|    min|                0.0|                0.0|\n",
      "|    max|                1.0|                1.0|\n",
      "+-------+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training_sum.predictions.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d37de0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "962faf5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_and_labels = fitted_churn_model.evaluate(test_churn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9d929f53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chandan/spark-3.2.4-bin-hadoop3.2/python/pyspark/sql/context.py:125: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "|            features|churn|       rawPrediction|         probability|prediction|\n",
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "|[22.0,11254.38,1....|    0|[4.68048104384972...|[0.99081067551387...|       0.0|\n",
      "|[27.0,8628.8,1.0,...|    0|[5.59372419041456...|[0.99629265179645...|       0.0|\n",
      "|[28.0,9090.43,1.0...|    0|[1.68911026087043...|[0.84410711468587...|       0.0|\n",
      "|[28.0,11204.23,0....|    0|[1.67114785096243...|[0.84172879948845...|       0.0|\n",
      "|[29.0,5900.78,1.0...|    0|[4.34026124077286...|[0.98713455396476...|       0.0|\n",
      "|[29.0,9617.59,0.0...|    0|[4.50410598647276...|[0.98905758449393...|       0.0|\n",
      "|[29.0,11274.46,1....|    0|[4.46482481185961...|[0.98862418677715...|       0.0|\n",
      "|[29.0,12711.15,0....|    0|[5.37067482905039...|[0.99537053995314...|       0.0|\n",
      "|[30.0,7960.64,1.0...|    1|[3.00761048291274...|[0.95291676122730...|       0.0|\n",
      "|[30.0,8677.28,1.0...|    0|[4.48586284396923...|[0.98885837298820...|       0.0|\n",
      "|[30.0,10183.98,1....|    0|[2.95465665973437...|[0.95048311428549...|       0.0|\n",
      "|[30.0,11575.37,1....|    1|[4.02072245703238...|[0.98237617208402...|       0.0|\n",
      "|[30.0,13473.35,0....|    0|[2.50354173172531...|[0.92438973585817...|       0.0|\n",
      "|[31.0,7073.61,0.0...|    0|[3.24644490543117...|[0.96254515500634...|       0.0|\n",
      "|[32.0,9885.12,1.0...|    1|[2.04077562438930...|[0.88501222340046...|       0.0|\n",
      "|[32.0,12142.99,0....|    0|[5.54737764080068...|[0.99611746787829...|       0.0|\n",
      "|[32.0,12479.72,0....|    0|[4.47457137964251...|[0.98873328020587...|       0.0|\n",
      "|[33.0,5738.82,0.0...|    0|[4.74415980219062...|[0.99137270725141...|       0.0|\n",
      "|[33.0,7750.54,1.0...|    0|[4.39568861272585...|[0.98781980003350...|       0.0|\n",
      "|[33.0,8556.73,0.0...|    0|[3.69789789955167...|[0.97582343527858...|       0.0|\n",
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_and_labels.predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a331af7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_eval = BinaryClassificationEvaluator(rawPredictionCol='prediction', labelCol='churn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "aa434931",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chandan/spark-3.2.4-bin-hadoop3.2/python/pyspark/sql/context.py:125: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "auc = churn_eval.evaluate(pred_and_labels.predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "72177354",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8064851814851814"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bac8946",
   "metadata": {},
   "source": [
    "### Lets evaluate on new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6ed6e835",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_lr_model = lr_churn.fit(final_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d3a53673",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_cust = spark.read.csv('new_customers.csv', inferSchema=True, header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "084f18f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Names: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- Total_Purchase: double (nullable = true)\n",
      " |-- Account_Manager: integer (nullable = true)\n",
      " |-- Years: double (nullable = true)\n",
      " |-- Num_Sites: double (nullable = true)\n",
      " |-- Onboard_date: string (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- Company: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_cust.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "fd77ddcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_new_customers = assembler.transform(new_cust)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "17895585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Names: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- Total_Purchase: double (nullable = true)\n",
      " |-- Account_Manager: integer (nullable = true)\n",
      " |-- Years: double (nullable = true)\n",
      " |-- Num_Sites: double (nullable = true)\n",
      " |-- Onboard_date: string (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- Company: string (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_new_customers.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e47d7935",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results =  final_lr_model.transform(test_new_customers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "8e6d3499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----+--------------+---------------+-----+---------+-------------------+--------------------+----------------+--------------------+--------------------+--------------------+----------+\n",
      "|         Names| Age|Total_Purchase|Account_Manager|Years|Num_Sites|       Onboard_date|            Location|         Company|            features|       rawPrediction|         probability|prediction|\n",
      "+--------------+----+--------------+---------------+-----+---------+-------------------+--------------------+----------------+--------------------+--------------------+--------------------+----------+\n",
      "| Andrew Mccall|37.0|       9935.53|              1| 7.71|      8.0|2011-08-29 18:37:54|38612 Johnny Stra...|        King Ltd|[37.0,9935.53,1.0...|[2.22168680572547...|[0.90218015921764...|       0.0|\n",
      "|Michele Wright|23.0|       7526.94|              1| 9.28|     15.0|2013-07-22 18:19:54|21083 Nicole Junc...|   Cannon-Benson|[23.0,7526.94,1.0...|[-6.2207539991844...|[0.00198380259784...|       1.0|\n",
      "|  Jeremy Chang|65.0|         100.0|              1|  1.0|     15.0|2006-12-11 07:48:13|085 Austin Views ...|Barron-Robertson|[65.0,100.0,1.0,1...|[-3.7691606662874...|[0.02255113312433...|       1.0|\n",
      "|Megan Ferguson|32.0|        6487.5|              0|  9.4|     14.0|2016-10-28 05:32:13|922 Wright Branch...|   Sexton-Golden|[32.0,6487.5,0.0,...|[-5.0956231362738...|[0.00608622076714...|       1.0|\n",
      "|  Taylor Young|32.0|      13147.71|              1| 10.0|      8.0|2012-03-20 00:36:46|Unit 0789 Box 073...|        Wood LLC|[32.0,13147.71,1....|[1.10475806383311...|[0.75115056144900...|       0.0|\n",
      "| Jessica Drake|22.0|       8445.26|              1| 3.46|     14.0|2011-02-04 19:29:27|1148 Tina Straven...|   Parks-Robbins|[22.0,8445.26,1.0...|[-1.6896020251072...|[0.15582818486369...|       1.0|\n",
      "+--------------+----+--------------+---------------+-----+---------+-------------------+--------------------+----------------+--------------------+--------------------+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_results.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "62316fba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------+\n",
      "|         Company|prediction|\n",
      "+----------------+----------+\n",
      "|        King Ltd|       0.0|\n",
      "|   Cannon-Benson|       1.0|\n",
      "|Barron-Robertson|       1.0|\n",
      "|   Sexton-Golden|       1.0|\n",
      "|        Wood LLC|       0.0|\n",
      "|   Parks-Robbins|       1.0|\n",
      "+----------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_results.select('Company', 'prediction').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de5e10a",
   "metadata": {},
   "source": [
    "Thank You!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0f950f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
