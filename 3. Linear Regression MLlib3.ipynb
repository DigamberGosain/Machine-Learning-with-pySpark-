{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10794b28",
   "metadata": {},
   "source": [
    "# <span style ='color:green;font-family:helvetica'>Linear Regression MLlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314829fe",
   "metadata": {},
   "source": [
    "### Linear Regression Documentation Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a275149",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0bfcba65",
   "metadata": {},
   "outputs": [],
   "source": [
    "findspark.init('/home/chandan/spark-3.2.4-bin-hadoop3.2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b701190d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19081295",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/18 17:30:16 WARN Utils: Your hostname, chandan-VivoBook-ASUSLaptop-X515MA-X515MA resolves to a loopback address: 127.0.1.1; using 192.168.0.169 instead (on interface wlo1)\n",
      "23/05/18 17:30:16 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/chandan/spark-3.2.4-bin-hadoop3.2/jars/spark-unsafe_2.12-3.2.4.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/05/18 17:30:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('lrex').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d020fb93",
   "metadata": {},
   "source": [
    "#### Import module form MLlib  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "038a4794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy\n",
      "  Downloading numpy-1.24.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 17.3 MB 483 kB/s eta 0:00:01    |█▋                              | 860 kB 1.2 MB/s eta 0:00:14     |███▉                            | 2.0 MB 1.2 MB/s eta 0:00:13     |████▌                           | 2.4 MB 1.2 MB/s eta 0:00:13     |█████▉                          | 3.1 MB 1.2 MB/s eta 0:00:12     |███████▏                        | 3.9 MB 4.5 MB/s eta 0:00:03     |███████████████▊                | 8.5 MB 825 kB/s eta 0:00:11     |███████████████████████▊        | 12.8 MB 1.6 MB/s eta 0:00:03     |█████████████████████████▏      | 13.6 MB 964 kB/s eta 0:00:04     |███████████████████████████▍    | 14.8 MB 851 kB/s eta 0:00:03\n",
      "\u001b[?25hInstalling collected packages: numpy\n",
      "Successfully installed numpy-1.24.3\n"
     ]
    }
   ],
   "source": [
    "!pip3 install  numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d8c1d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "43bbecb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/18 17:38:14 WARN LibSVMFileFormat: 'numFeatures' option not specified, determining the number of features by going though the input. If you know the number in advance, please specify it via 'numFeatures' option to avoid the extra scan.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "training = spark.read.format('libsvm').load('sample_linear_regression_data.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cdfdb79b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+\n",
      "|              label|            features|\n",
      "+-------------------+--------------------+\n",
      "| -9.490009878824548|(10,[0,1,2,3,4,5,...|\n",
      "| 0.2577820163584905|(10,[0,1,2,3,4,5,...|\n",
      "| -4.438869807456516|(10,[0,1,2,3,4,5,...|\n",
      "|-19.782762789614537|(10,[0,1,2,3,4,5,...|\n",
      "| -7.966593841555266|(10,[0,1,2,3,4,5,...|\n",
      "| -7.896274316726144|(10,[0,1,2,3,4,5,...|\n",
      "| -8.464803554195287|(10,[0,1,2,3,4,5,...|\n",
      "| 2.1214592666251364|(10,[0,1,2,3,4,5,...|\n",
      "| 1.0720117616524107|(10,[0,1,2,3,4,5,...|\n",
      "|-13.772441561702871|(10,[0,1,2,3,4,5,...|\n",
      "| -5.082010756207233|(10,[0,1,2,3,4,5,...|\n",
      "|  7.887786536531237|(10,[0,1,2,3,4,5,...|\n",
      "| 14.323146365332388|(10,[0,1,2,3,4,5,...|\n",
      "|-20.057482615789212|(10,[0,1,2,3,4,5,...|\n",
      "|-0.8995693247765151|(10,[0,1,2,3,4,5,...|\n",
      "| -19.16829262296376|(10,[0,1,2,3,4,5,...|\n",
      "|  5.601801561245534|(10,[0,1,2,3,4,5,...|\n",
      "|-3.2256352187273354|(10,[0,1,2,3,4,5,...|\n",
      "| 1.5299675726687754|(10,[0,1,2,3,4,5,...|\n",
      "| -0.250102447941961|(10,[0,1,2,3,4,5,...|\n",
      "+-------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7cbf29e",
   "metadata": {},
   "source": [
    "#### This is the format which MLlib works with"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e19bbc7",
   "metadata": {},
   "source": [
    "#### Now let's create an instance of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06808896",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression( featuresCol='features', labelCol='label', predictionCol='prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "58f7b3ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/18 17:42:50 WARN Instrumentation: [ee743365] regParam is zero, which might cause numerical instability and overfitting.\n",
      "23/05/18 17:42:54 WARN InstanceBuilder$NativeLAPACK: Failed to load implementation from:dev.ludovic.netlib.lapack.JNILAPACK\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "lrModel = lr.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b3ee9a5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([0.0073, 0.8314, -0.8095, 2.4412, 0.5192, 1.1535, -0.2989, -0.5129, -0.6197, 0.6956])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrModel.coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "52bd118d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14228558260358093"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrModel.intercept"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6db778a",
   "metadata": {},
   "source": [
    "#### After fitting the model on training data we can grab coefficients and intercept"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7d8cea",
   "metadata": {},
   "source": [
    "#### We can see the summary for the following information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5ac66851",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_summary = lrModel.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "224ea171",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.027839179518600154"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_summary.r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b7b5e9a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.16309157133015"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_summary.rootMeanSquaredError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a2f647",
   "metadata": {},
   "source": [
    "#### Right now we did a mistake we didn't do the Train test split, let's see how we do that with pySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ada97881",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/18 17:50:51 WARN LibSVMFileFormat: 'numFeatures' option not specified, determining the number of features by going though the input. If you know the number in advance, please specify it via 'numFeatures' option to avoid the extra scan.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "all_data = spark.read.format('libsvm').load('sample_linear_regression_data.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ef751f",
   "metadata": {},
   "source": [
    "#### We use random split and pass on list of percentage of data we want to splitted into"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0962521e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = all_data.randomSplit([0.7,0.30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f6b15231",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 5:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+\n",
      "|              label|            features|\n",
      "+-------------------+--------------------+\n",
      "|-28.571478869743427|(10,[0,1,2,3,4,5,...|\n",
      "|-28.046018037776633|(10,[0,1,2,3,4,5,...|\n",
      "|-26.805483428483072|(10,[0,1,2,3,4,5,...|\n",
      "|-26.736207182601724|(10,[0,1,2,3,4,5,...|\n",
      "| -23.51088409032297|(10,[0,1,2,3,4,5,...|\n",
      "|-23.487440120936512|(10,[0,1,2,3,4,5,...|\n",
      "|-22.949825936196074|(10,[0,1,2,3,4,5,...|\n",
      "|-22.837460416919342|(10,[0,1,2,3,4,5,...|\n",
      "|-21.432387764165806|(10,[0,1,2,3,4,5,...|\n",
      "|-19.884560774273424|(10,[0,1,2,3,4,5,...|\n",
      "|-19.872991038068406|(10,[0,1,2,3,4,5,...|\n",
      "|-19.782762789614537|(10,[0,1,2,3,4,5,...|\n",
      "| -19.66731861537172|(10,[0,1,2,3,4,5,...|\n",
      "|-19.402336030214553|(10,[0,1,2,3,4,5,...|\n",
      "|-18.845922472898582|(10,[0,1,2,3,4,5,...|\n",
      "| -18.27521356600463|(10,[0,1,2,3,4,5,...|\n",
      "|-17.065399625876015|(10,[0,1,2,3,4,5,...|\n",
      "|-17.026492264209548|(10,[0,1,2,3,4,5,...|\n",
      "|-16.692207021311106|(10,[0,1,2,3,4,5,...|\n",
      "| -16.26143027545273|(10,[0,1,2,3,4,5,...|\n",
      "+-------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "train_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4e440202",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+\n",
      "|summary|              label|\n",
      "+-------+-------------------+\n",
      "|  count|                343|\n",
      "|   mean| 0.2929441564343827|\n",
      "| stddev| 10.461277633179497|\n",
      "|    min|-28.571478869743427|\n",
      "|    max|  27.78383192005107|\n",
      "+-------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "980ba3d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+\n",
      "|summary|              label|\n",
      "+-------+-------------------+\n",
      "|  count|                158|\n",
      "|   mean|0.17861679913098988|\n",
      "| stddev| 10.031810165562874|\n",
      "|    min|-20.212077258958672|\n",
      "|    max| 27.111027963108548|\n",
      "+-------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_data.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a16983fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/18 17:55:00 WARN Instrumentation: [a798101a] regParam is zero, which might cause numerical instability and overfitting.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "corrected_model = lr.fit(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0f19c7",
   "metadata": {},
   "source": [
    "### We have trained the model on training set now evaluate on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "24332bc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "test_results = corrected_model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b9d37a4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chandan/spark-3.2.4-bin-hadoop3.2/python/pyspark/sql/context.py:125: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n",
      "[Stage 15:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|          residuals|\n",
      "+-------------------+\n",
      "|-22.145583219441228|\n",
      "|-20.856309315189794|\n",
      "|-17.498805336981338|\n",
      "|-17.533668755980585|\n",
      "| -17.29029100979777|\n",
      "|-17.683469809596023|\n",
      "|-16.982881286507368|\n",
      "| -16.47627840659628|\n",
      "| -15.07677818880474|\n",
      "| -16.12806261642478|\n",
      "|-14.283708963649808|\n",
      "|-14.759337662791651|\n",
      "|-17.428880116458945|\n",
      "|-13.309909598706215|\n",
      "|-13.711789965830928|\n",
      "|-12.152860923583875|\n",
      "|-16.374820976519118|\n",
      "|-10.163425853612356|\n",
      "| -8.993906488532183|\n",
      "|-12.292049386703459|\n",
      "+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "test_results.residuals.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0bef70c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.948260440924553"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_results.rootMeanSquaredError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90c3b70",
   "metadata": {},
   "source": [
    "### Let's see how to deploy the model, it's a mimic of the process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e96ab2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabeled_data = test_data.select('features')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf5fab1",
   "metadata": {},
   "source": [
    "### We have created the unlabeld data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3e79bc26",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = corrected_model.transform(unlabeled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f1cf46ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 16:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|            features|          prediction|\n",
      "+--------------------+--------------------+\n",
      "|(10,[0,1,2,3,4,5,...|  1.9335059604825564|\n",
      "|(10,[0,1,2,3,4,5,...|   0.798826699400583|\n",
      "|(10,[0,1,2,3,4,5,...|  -1.669487285982425|\n",
      "|(10,[0,1,2,3,4,5,...| -0.2699574326839303|\n",
      "|(10,[0,1,2,3,4,5,...|  -0.203909347085573|\n",
      "|(10,[0,1,2,3,4,5,...|  0.2547952386565173|\n",
      "|(10,[0,1,2,3,4,5,...| -0.3438394461685798|\n",
      "|(10,[0,1,2,3,4,5,...|-0.24281842700881012|\n",
      "|(10,[0,1,2,3,4,5,...| -1.0088808522167507|\n",
      "|(10,[0,1,2,3,4,5,...| 0.17655005063020757|\n",
      "|(10,[0,1,2,3,4,5,...| -1.0921487596624884|\n",
      "|(10,[0,1,2,3,4,5,...| -0.5895334925876015|\n",
      "|(10,[0,1,2,3,4,5,...|   2.372397141916513|\n",
      "|(10,[0,1,2,3,4,5,...| -0.6662213324464876|\n",
      "|(10,[0,1,2,3,4,5,...| 0.29119518994017046|\n",
      "|(10,[0,1,2,3,4,5,...| -0.8870671405207401|\n",
      "|(10,[0,1,2,3,4,5,...|  3.4525978731486977|\n",
      "|(10,[0,1,2,3,4,5,...|   -2.60980114563884|\n",
      "|(10,[0,1,2,3,4,5,...| -3.4170399143079786|\n",
      "|(10,[0,1,2,3,4,5,...| 0.09395282204204729|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "predictions.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a8bb61",
   "metadata": {},
   "source": [
    "#### The above is the workflow of MLlib with pySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e327459",
   "metadata": {},
   "source": [
    "# Regression Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7abb92",
   "metadata": {},
   "source": [
    "Regression common evaluation metrics are:\n",
    "* Mean Absolute Error\n",
    "* Mean Squared Error\n",
    "* Root Mean Square Error\n",
    "* R squred Values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ee8825",
   "metadata": {},
   "source": [
    "#### R Squared Values also know as coefficents of determination\n",
    "* It is not quite an error metric, more of a statistical measure of regression model\n",
    "* In a basic sense it is a measure of how much variance you model accounts for.\n",
    "* Between 0-1(0% to 100%).\n",
    "* There are also different ways of R2, such as adjusted R squared and used for comaring models\n",
    "* Its basically a percentage of how much variabce is explained"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5693c9b3",
   "metadata": {},
   "source": [
    "### Practicle Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ed2b0a",
   "metadata": {},
   "source": [
    "We work with some real time data of Ecommerce Customer Data for a company's website and mobile app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e82fc726",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8cadb415",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 18:>                                                         (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data = spark.read.csv('Ecommerce_Customers.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b1895575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Email: string (nullable = true)\n",
      " |-- Address: string (nullable = true)\n",
      " |-- Avatar: string (nullable = true)\n",
      " |-- Avg Session Length: double (nullable = true)\n",
      " |-- Time on App: double (nullable = true)\n",
      " |-- Time on Website: double (nullable = true)\n",
      " |-- Length of Membership: double (nullable = true)\n",
      " |-- Yearly Amount Spent: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4019c809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hduke@hotmail.com\n",
      "4547 Archer CommonDiazchester, CA 06566-8576\n",
      "DarkGreen\n",
      "31.92627202636016\n",
      "11.109460728682564\n",
      "37.268958868297744\n",
      "2.66403418213262\n",
      "392.2049334443264\n"
     ]
    }
   ],
   "source": [
    "for item in data.head(2)[1]:\n",
    "    print(item)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9f2107",
   "metadata": {},
   "source": [
    " ### The  most important step is to set up data for Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0b054a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5f91450f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Email',\n",
       " 'Address',\n",
       " 'Avatar',\n",
       " 'Avg Session Length',\n",
       " 'Time on App',\n",
       " 'Time on Website',\n",
       " 'Length of Membership',\n",
       " 'Yearly Amount Spent']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f0fdcc87",
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=['Avg Session Length',\n",
    "                                         'Time on App','Time on Website',\n",
    "                                         'Length of Membership'],outputCol='features' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "81a73fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = assembler.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5f4e6186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Email: string (nullable = true)\n",
      " |-- Address: string (nullable = true)\n",
      " |-- Avatar: string (nullable = true)\n",
      " |-- Avg Session Length: double (nullable = true)\n",
      " |-- Time on App: double (nullable = true)\n",
      " |-- Time on Website: double (nullable = true)\n",
      " |-- Length of Membership: double (nullable = true)\n",
      " |-- Yearly Amount Spent: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d48cfd35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            features|\n",
      "+--------------------+\n",
      "|[34.4972677251122...|\n",
      "|[31.9262720263601...|\n",
      "|[33.0009147556426...|\n",
      "|[34.3055566297555...|\n",
      "|[33.3306725236463...|\n",
      "|[33.8710378793419...|\n",
      "|[32.0215955013870...|\n",
      "|[32.7391429383803...|\n",
      "|[33.9877728956856...|\n",
      "|[31.9365486184489...|\n",
      "|[33.9925727749537...|\n",
      "|[33.8793608248049...|\n",
      "|[29.5324289670579...|\n",
      "|[33.1903340437226...|\n",
      "|[32.3879758531538...|\n",
      "|[30.7377203726281...|\n",
      "|[32.1253868972878...|\n",
      "|[32.3388993230671...|\n",
      "|[32.1878120459321...|\n",
      "|[32.6178560628234...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output.select('features').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8725b2c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Email='mstephenson@fernandez.com', Address='835 Frank TunnelWrightmouth, MI 82180-9605', Avatar='Violet', Avg Session Length=34.49726772511229, Time on App=12.65565114916675, Time on Website=39.57766801952616, Length of Membership=4.0826206329529615, Yearly Amount Spent=587.9510539684005, features=DenseVector([34.4973, 12.6557, 39.5777, 4.0826]))]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e0cc00",
   "metadata": {},
   "source": [
    "#### As we can see with fratures now we have created one dense feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "733f5337",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = output.select([\"features\",\"Yearly Amount Spent\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "35d1f91a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+\n",
      "|            features|Yearly Amount Spent|\n",
      "+--------------------+-------------------+\n",
      "|[34.4972677251122...|  587.9510539684005|\n",
      "|[31.9262720263601...|  392.2049334443264|\n",
      "|[33.0009147556426...| 487.54750486747207|\n",
      "|[34.3055566297555...|  581.8523440352177|\n",
      "|[33.3306725236463...|  599.4060920457634|\n",
      "|[33.8710378793419...|   637.102447915074|\n",
      "|[32.0215955013870...|  521.5721747578274|\n",
      "|[32.7391429383803...|  549.9041461052942|\n",
      "|[33.9877728956856...|  570.2004089636196|\n",
      "|[31.9365486184489...|  427.1993848953282|\n",
      "|[33.9925727749537...|  492.6060127179966|\n",
      "|[33.8793608248049...|  522.3374046069357|\n",
      "|[29.5324289670579...|  408.6403510726275|\n",
      "|[33.1903340437226...|  573.4158673313865|\n",
      "|[32.3879758531538...|  470.4527333009554|\n",
      "|[30.7377203726281...|  461.7807421962299|\n",
      "|[32.1253868972878...| 457.84769594494855|\n",
      "|[32.3388993230671...| 407.70454754954415|\n",
      "|[32.1878120459321...|  452.3156754800354|\n",
      "|[32.6178560628234...|   605.061038804892|\n",
      "+--------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989c5174",
   "metadata": {},
   "source": [
    "### Now we have one features which a dense vector of all features and label. Next we will do train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "54345425",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = final_data.randomSplit([.70,.30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9fa1e53e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+\n",
      "|summary|Yearly Amount Spent|\n",
      "+-------+-------------------+\n",
      "|  count|                347|\n",
      "|   mean|  496.5839067200225|\n",
      "| stddev|  80.17615682216228|\n",
      "|    min|   266.086340948469|\n",
      "|    max|  744.2218671047146|\n",
      "+-------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "464f955b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+\n",
      "|summary|Yearly Amount Spent|\n",
      "+-------+-------------------+\n",
      "|  count|                153|\n",
      "|   mean|  505.5059052120757|\n",
      "| stddev|   77.2262298406661|\n",
      "|    min| 256.67058229005585|\n",
      "|    max|  765.5184619388373|\n",
      "+-------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_data.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f187c547",
   "metadata": {},
   "source": [
    "#### Working with Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4b66a7",
   "metadata": {},
   "source": [
    "## Linear Regression Parameters\n",
    "`class pyspark.ml.regression.LinearRegression(*, featuresCol: str = 'features', labelCol: str = 'label', predictionCol: str = 'prediction', maxIter: int = 100, regParam: float = 0.0, elasticNetParam: float = 0.0, tol: float = 1e-06, fitIntercept: bool = True, standardization: bool = True, solver: str = 'auto', weightCol: Optional[str] = None, aggregationDepth: int = 2, loss: str = 'squaredError', epsilon: float = 1.35, maxBlockSizeInMB: float = 0.0)'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec60b8fc",
   "metadata": {},
   "source": [
    "With the paramteters of Linear Regression we can see we have to define featureCol: , Labelcol:, predictionCol we can use the default text values till the time our columns name matching with it, but in our example case our label column is `Yearly Amount Spent`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "55a07b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression(labelCol= 'Yearly Amount Spent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fa3f86c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/19 14:47:00 WARN Instrumentation: [dd369e3c] regParam is zero, which might cause numerical instability and overfitting.\n",
      "23/05/19 14:47:01 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "23/05/19 14:47:01 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.ForeignLinkerBLAS\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "lt_model = lr.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "85ee8269",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = lt_model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0176d7ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chandan/spark-3.2.4-bin-hadoop3.2/python/pyspark/sql/context.py:125: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|          residuals|\n",
      "+-------------------+\n",
      "| -7.002638459494392|\n",
      "|  18.36659390073015|\n",
      "| -8.775637574984216|\n",
      "| -9.265082160524514|\n",
      "|-1.7790075400681644|\n",
      "|  -4.00530604292436|\n",
      "|-1.7796351569165267|\n",
      "| -5.502762223509478|\n",
      "|   2.00954663087947|\n",
      "|  -4.68217741428731|\n",
      "|-11.303471619427683|\n",
      "| -9.162029578429326|\n",
      "| -9.380933187718426|\n",
      "| 11.324235896040932|\n",
      "|-13.870245000507566|\n",
      "|  -8.55589385602002|\n",
      "|  4.410254180431252|\n",
      "|  17.10186381942657|\n",
      "|  22.62711852838771|\n",
      "| 17.146542422299603|\n",
      "+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_results.residuals.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "23a303e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.846491681110335"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_results.rootMeanSquaredError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8f3a4e71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9836363111466025"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_results.r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a721cd95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+\n",
      "|summary|Yearly Amount Spent|\n",
      "+-------+-------------------+\n",
      "|  count|                500|\n",
      "|   mean|  499.3140382585909|\n",
      "| stddev|   79.3147815497068|\n",
      "|    min| 256.67058229005585|\n",
      "|    max|  765.5184619388373|\n",
      "+-------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_data.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3eb4132d",
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabeled_data = test_data.select(\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f7fe4c1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            features|\n",
      "+--------------------+\n",
      "|[31.1280900496166...|\n",
      "|[31.3123495994443...|\n",
      "|[31.4474464941278...|\n",
      "|[31.5261978982398...|\n",
      "|[31.5761319713222...|\n",
      "|[31.6253601348306...|\n",
      "|[31.7216523605090...|\n",
      "|[31.7242025238451...|\n",
      "|[31.7366356860502...|\n",
      "|[31.7656188210424...|\n",
      "|[31.8093003166791...|\n",
      "|[31.8279790554652...|\n",
      "|[31.8854062999117...|\n",
      "|[31.9096268275227...|\n",
      "|[31.9365486184489...|\n",
      "|[32.0085045178551...|\n",
      "|[32.0215955013870...|\n",
      "|[32.0478146331398...|\n",
      "|[32.0498393904573...|\n",
      "|[32.0609143984100...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unlabeled_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f940af",
   "metadata": {},
   "source": [
    "#### For the purpose of Deployment we will use .transform on the new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "cef65f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = lt_model.transform(unlabeled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b16ba16c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+\n",
      "|            features|        prediction|\n",
      "+--------------------+------------------+\n",
      "|[31.1280900496166...| 564.2553252065491|\n",
      "|[31.3123495994443...|445.22482412721047|\n",
      "|[31.4474464941278...|427.37837967020823|\n",
      "|[31.5261978982398...| 418.3596083528623|\n",
      "|[31.5761319713222...| 543.0055915293965|\n",
      "|[31.6253601348306...|380.34220679984855|\n",
      "|[31.7216523605090...|349.55656178878917|\n",
      "|[31.7242025238451...|   508.89064951147|\n",
      "|[31.7366356860502...|494.92389962465245|\n",
      "|[31.7656188210424...|501.23625904989444|\n",
      "|[31.8093003166791...| 548.0753709822689|\n",
      "|[31.8279790554652...|449.16477712537085|\n",
      "|[31.8854062999117...|399.48420616019393|\n",
      "|[31.9096268275227...| 552.1217997771982|\n",
      "|[31.9365486184489...|441.06962989583576|\n",
      "|[32.0085045178551...| 451.7531148847754|\n",
      "|[32.0215955013870...| 517.1619205773961|\n",
      "|[32.0478146331398...|480.28769393941684|\n",
      "|[32.0498393904573...| 456.0922383458276|\n",
      "|[32.0609143984100...| 610.4567762907154|\n",
      "+--------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd91a16",
   "metadata": {},
   "source": [
    "## Exercise Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7dc6a5a",
   "metadata": {},
   "source": [
    "* Scenario: You have been contracted by Hyndai Heavy Industries to help them build a predictive model for some ships\n",
    "\n",
    "* Problem: They need accurate estimates of how many crew members a ship will require.\n",
    "\n",
    "* Information: They are Currently selling ships to some new customers and want you to create a model and use it to      predict how many crew members the shio will need.\n",
    "\n",
    "* Data: They provide you data with these features\n",
    "    * Ship Name\n",
    "    * Cruise Line\n",
    "    * Age (as of 2013)\n",
    "    * Tonnage (1000's of tons)\n",
    "    * Passengers(100's)\n",
    "    * Length (100's of feet)\n",
    "    * Cabins (100's)\n",
    "    * Passenger Density\n",
    "    * Crew(100's)\n",
    "    \n",
    "* The clinet also mentioned that they have found that particular cruise lines will differ in acceptable crew counts, so it is most likely an important feature to include in your analysis! We can use `Stringindexer for the documentation'\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10adaf2",
   "metadata": {},
   "source": [
    "### Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a3a56443",
   "metadata": {},
   "outputs": [],
   "source": [
    "cruise_data = spark.read.csv('cruise_ship_info.csv', inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4c76d796",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+-----------+------------------+------------------+-----------------+-----------------+------------------+-----------------+-----------------+\n",
      "|summary|Ship_name|Cruise_line|               Age|           Tonnage|       passengers|           length|            cabins|passenger_density|             crew|\n",
      "+-------+---------+-----------+------------------+------------------+-----------------+-----------------+------------------+-----------------+-----------------+\n",
      "|  count|      158|        158|               158|               158|              158|              158|               158|              158|              158|\n",
      "|   mean| Infinity|       null|15.689873417721518| 71.28467088607599|18.45740506329114|8.130632911392404| 8.830000000000005|39.90094936708861|7.794177215189873|\n",
      "| stddev|     null|       null| 7.615691058751413|37.229540025907866|9.677094775143416|1.793473548054825|4.4714172221480615| 8.63921711391542|3.503486564627034|\n",
      "|    min|Adventure|    Azamara|                 4|             2.329|             0.66|             2.79|              0.33|             17.7|             0.59|\n",
      "|    max|Zuiderdam|   Windstar|                48|             220.0|             54.0|            11.82|              27.0|            71.43|             21.0|\n",
      "+-------+---------+-----------+------------------+------------------+-----------------+-----------------+------------------+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cruise_data.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d13fb5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import isnull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a1294354",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ship_name',\n",
       " 'Cruise_line',\n",
       " 'Age',\n",
       " 'Tonnage',\n",
       " 'passengers',\n",
       " 'length',\n",
       " 'cabins',\n",
       " 'passenger_density',\n",
       " 'crew']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cruise_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "7aba7c11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+---+-------+----------+------+------+-----------------+----+\n",
      "|Ship_name|Cruise_line|Age|Tonnage|passengers|length|cabins|passenger_density|crew|\n",
      "+---------+-----------+---+-------+----------+------+------+-----------------+----+\n",
      "+---------+-----------+---+-------+----------+------+------+-----------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cruise_data.filter('crew IS NULL').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386c2ebf",
   "metadata": {},
   "source": [
    "### Checked null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "55c2a5aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(Ship_name='Journey', Cruise_line='Azamara', Age=6, Tonnage=30.276999999999997, passengers=6.94, length=5.94, cabins=3.55, passenger_density=42.64, crew=3.55)\n",
      "\n",
      "\n",
      "Row(Ship_name='Quest', Cruise_line='Azamara', Age=6, Tonnage=30.276999999999997, passengers=6.94, length=5.94, cabins=3.55, passenger_density=42.64, crew=3.55)\n",
      "\n",
      "\n",
      "Row(Ship_name='Celebration', Cruise_line='Carnival', Age=26, Tonnage=47.262, passengers=14.86, length=7.22, cabins=7.43, passenger_density=31.8, crew=6.7)\n",
      "\n",
      "\n",
      "Row(Ship_name='Conquest', Cruise_line='Carnival', Age=11, Tonnage=110.0, passengers=29.74, length=9.53, cabins=14.88, passenger_density=36.99, crew=19.1)\n",
      "\n",
      "\n",
      "Row(Ship_name='Destiny', Cruise_line='Carnival', Age=17, Tonnage=101.353, passengers=26.42, length=8.92, cabins=13.21, passenger_density=38.36, crew=10.0)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for ship in cruise_data.head(5):\n",
    "    print(ship)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb24c2be",
   "metadata": {},
   "source": [
    "Let's convert Cruise line into a number "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "06d36a10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 62:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----+\n",
      "|      Cruise_line|count|\n",
      "+-----------------+-----+\n",
      "|            Costa|   11|\n",
      "|              P&O|    6|\n",
      "|           Cunard|    3|\n",
      "|Regent_Seven_Seas|    5|\n",
      "|              MSC|    8|\n",
      "|         Carnival|   22|\n",
      "|          Crystal|    2|\n",
      "|           Orient|    1|\n",
      "|         Princess|   17|\n",
      "|        Silversea|    4|\n",
      "|         Seabourn|    3|\n",
      "| Holland_American|   14|\n",
      "|         Windstar|    3|\n",
      "|           Disney|    2|\n",
      "|        Norwegian|   13|\n",
      "|          Oceania|    3|\n",
      "|          Azamara|    2|\n",
      "|        Celebrity|   10|\n",
      "|             Star|    6|\n",
      "|  Royal_Caribbean|   23|\n",
      "+-----------------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "cruise_data.groupBy(\"Cruise_line\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "40569855",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "37bcd715",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(Ship_name='Journey', Cruise_line='Azamara', Age=6, Tonnage=30.276999999999997, passengers=6.94, length=5.94, cabins=3.55, passenger_density=42.64, crew=3.55, Cruise_cat=16.0)]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexer = StringIndexer(inputCol='Cruise_line', outputCol='Cruise_cat')\n",
    "indexed = indexer.fit(cruise_data).transform(cruise_data)\n",
    "indexed.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "f657e42c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Ship_name='Journey', Cruise_line='Azamara', Age=6, Tonnage=30.276999999999997, passengers=6.94, length=5.94, cabins=3.55, passenger_density=42.64, crew=3.55, Cruise_cat=16.0),\n",
       " Row(Ship_name='Quest', Cruise_line='Azamara', Age=6, Tonnage=30.276999999999997, passengers=6.94, length=5.94, cabins=3.55, passenger_density=42.64, crew=3.55, Cruise_cat=16.0),\n",
       " Row(Ship_name='Celebration', Cruise_line='Carnival', Age=26, Tonnage=47.262, passengers=14.86, length=7.22, cabins=7.43, passenger_density=31.8, crew=6.7, Cruise_cat=1.0),\n",
       " Row(Ship_name='Conquest', Cruise_line='Carnival', Age=11, Tonnage=110.0, passengers=29.74, length=9.53, cabins=14.88, passenger_density=36.99, crew=19.1, Cruise_cat=1.0),\n",
       " Row(Ship_name='Destiny', Cruise_line='Carnival', Age=17, Tonnage=101.353, passengers=26.42, length=8.92, cabins=13.21, passenger_density=38.36, crew=10.0, Cruise_cat=1.0)]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexed.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "648f1611",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "5f9580be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ship_name',\n",
       " 'Cruise_line',\n",
       " 'Age',\n",
       " 'Tonnage',\n",
       " 'passengers',\n",
       " 'length',\n",
       " 'cabins',\n",
       " 'passenger_density',\n",
       " 'crew',\n",
       " 'Cruise_cat']"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexed.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "d7c69681",
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler( inputCols=['Age',\n",
    " 'Tonnage',\n",
    " 'passengers',\n",
    " 'length',\n",
    " 'cabins',\n",
    " 'passenger_density',\n",
    " 'Cruise_cat'], outputCol= 'features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "e21dba3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = assembler.transform(indexed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "b17af6bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+\n",
      "|            features|crew|\n",
      "+--------------------+----+\n",
      "|[6.0,30.276999999...|3.55|\n",
      "|[6.0,30.276999999...|3.55|\n",
      "|[26.0,47.262,14.8...| 6.7|\n",
      "|[11.0,110.0,29.74...|19.1|\n",
      "|[17.0,101.353,26....|10.0|\n",
      "|[22.0,70.367,20.5...| 9.2|\n",
      "|[15.0,70.367,20.5...| 9.2|\n",
      "|[23.0,70.367,20.5...| 9.2|\n",
      "|[19.0,70.367,20.5...| 9.2|\n",
      "|[6.0,110.23899999...|11.5|\n",
      "|[10.0,110.0,29.74...|11.6|\n",
      "|[28.0,46.052,14.5...| 6.6|\n",
      "|[18.0,70.367,20.5...| 9.2|\n",
      "|[17.0,70.367,20.5...| 9.2|\n",
      "|[11.0,86.0,21.24,...| 9.3|\n",
      "|[8.0,110.0,29.74,...|11.6|\n",
      "|[9.0,88.5,21.24,9...|10.3|\n",
      "|[15.0,70.367,20.5...| 9.2|\n",
      "|[12.0,88.5,21.24,...| 9.3|\n",
      "|[20.0,70.367,20.5...| 9.2|\n",
      "+--------------------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output.select(['features','crew']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "c9ce5a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = output.select(['features','crew'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4a2138",
   "metadata": {},
   "source": [
    "### Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "8811cc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = final_data.randomSplit([0.7,0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "4c6e054a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|              crew|\n",
      "+-------+------------------+\n",
      "|  count|               108|\n",
      "|   mean| 7.796111111111122|\n",
      "| stddev|3.4693627935323255|\n",
      "|    min|              0.59|\n",
      "|    max|              19.1|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "4143f651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|              crew|\n",
      "+-------+------------------+\n",
      "|  count|                50|\n",
      "|   mean|7.7899999999999965|\n",
      "| stddev|3.6117133703254685|\n",
      "|    min|              0.59|\n",
      "|    max|              21.0|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_data.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "06263976",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "d276cc9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ship_lr = LinearRegression(labelCol='crew')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "6774144f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/19 16:18:59 WARN Instrumentation: [e6c9c04c] regParam is zero, which might cause numerical instability and overfitting.\n"
     ]
    }
   ],
   "source": [
    "trained_ship_model = ship_lr.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "1b418b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ship_results = trained_ship_model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "23de18a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6092536191487219"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ship_results.rootMeanSquaredError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "d1e2cdbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|              crew|\n",
      "+-------+------------------+\n",
      "|  count|               108|\n",
      "|   mean| 7.796111111111122|\n",
      "| stddev|3.4693627935323255|\n",
      "|    min|              0.59|\n",
      "|    max|              19.1|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "c604fe97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9709635441480405"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ship_results.r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "2c562ad1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.37118997244581586"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ship_results.meanSquaredError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "ccad1f90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.48361984391183055"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ship_results.meanAbsoluteError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f89a703",
   "metadata": {},
   "source": [
    "### As the results are of evaluation matrix are good we should check theh correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "8840edca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "9e117e54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+\n",
      "|corr(crew, passengers)|\n",
      "+----------------------+\n",
      "|    0.9152341306065384|\n",
      "+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cruise_data.select(corr('crew','passengers')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "8b06cbc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|corr(crew, cabins)|\n",
      "+------------------+\n",
      "|0.9508226063578497|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cruise_data.select(corr('crew','cabins')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2599868",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
