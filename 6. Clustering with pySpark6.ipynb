{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78adf5b3",
   "metadata": {},
   "source": [
    "# <span style ='color:brown;font-family: helvetica'> K-means Clustering with pySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "210862ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90c189cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "findspark.init('/home/chandan/spark-3.2.4-bin-hadoop3.2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ba36c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "252ad0f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/21 12:46:56 WARN Utils: Your hostname, chandan-VivoBook-ASUSLaptop-X515MA-X515MA resolves to a loopback address: 127.0.1.1; using 192.168.0.169 instead (on interface wlo1)\n",
      "23/05/21 12:46:56 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/chandan/spark-3.2.4-bin-hadoop3.2/jars/spark-unsafe_2.12-3.2.4.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/05/21 12:46:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('clustering').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "facbab61",
   "metadata": {},
   "source": [
    "### Documentation Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b9340a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7de410d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/21 13:04:26 WARN LibSVMFileFormat: 'numFeatures' option not specified, determining the number of features by going though the input. If you know the number in advance, please specify it via 'numFeatures' option to avoid the extra scan.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "dataset = spark.read.format('libsvm').load(\"sample_kmeans_data.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "443fdafc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|  0.0|           (3,[],[])|\n",
      "|  1.0|(3,[0,1,2],[0.1,0...|\n",
      "|  2.0|(3,[0,1,2],[0.2,0...|\n",
      "|  3.0|(3,[0,1,2],[9.0,9...|\n",
      "|  4.0|(3,[0,1,2],[9.1,9...|\n",
      "|  5.0|(3,[0,1,2],[9.2,9...|\n",
      "+-----+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "dataset.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1af3e39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = dataset.select('features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd094fd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            features|\n",
      "+--------------------+\n",
      "|           (3,[],[])|\n",
      "|(3,[0,1,2],[0.1,0...|\n",
      "|(3,[0,1,2],[0.2,0...|\n",
      "|(3,[0,1,2],[9.0,9...|\n",
      "|(3,[0,1,2],[9.1,9...|\n",
      "|(3,[0,1,2],[9.2,9...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "151adf5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans  = KMeans().setK(2).setSeed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4a54ce",
   "metadata": {},
   "source": [
    "#### setSeed is the seed value to a random number generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9edde7fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "model = kmeans.fit(final_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b02e66b",
   "metadata": {},
   "source": [
    "### We can do model evaluation with within set sum of squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c1f674d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "centers = model.clusterCenters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c78e05f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([9.1, 9.1, 9.1]), array([0.1, 0.1, 0.1])]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "70123c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.transform(final_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8cdfff1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|            features|prediction|\n",
      "+--------------------+----------+\n",
      "|           (3,[],[])|         1|\n",
      "|(3,[0,1,2],[0.1,0...|         1|\n",
      "|(3,[0,1,2],[0.2,0...|         1|\n",
      "|(3,[0,1,2],[9.0,9...|         0|\n",
      "|(3,[0,1,2],[9.1,9...|         0|\n",
      "|(3,[0,1,2],[9.2,9...|         0|\n",
      "+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3b063c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import ClusteringEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e083c487",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = ClusteringEvaluator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a2795436",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "clus_eval = evaluator.evaluate(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b51fed6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9997530305375207"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clus_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d328a659",
   "metadata": {},
   "outputs": [],
   "source": [
    "wsse = model.summary.trainingCost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ac0e81ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11999999999994547"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wsse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6c4a98",
   "metadata": {},
   "source": [
    "### Now We will See Practice Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd7ed53",
   "metadata": {},
   "source": [
    "# Practice Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ce24d0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"seeds_dataset.csv\", header= True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9bdefd2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- area: double (nullable = true)\n",
      " |-- perimeter: double (nullable = true)\n",
      " |-- compactness: double (nullable = true)\n",
      " |-- length_of_kernel: double (nullable = true)\n",
      " |-- width_of_kernel: double (nullable = true)\n",
      " |-- asymmetry_coefficient: double (nullable = true)\n",
      " |-- length_of_groove: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8cac7de0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 24:>                                                         (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(area=15.26, perimeter=14.84, compactness=0.871, length_of_kernel=5.763, width_of_kernel=3.312, asymmetry_coefficient=2.221, length_of_groove=5.22)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c031d4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "994d6915",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bd5d55a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['area',\n",
       " 'perimeter',\n",
       " 'compactness',\n",
       " 'length_of_kernel',\n",
       " 'width_of_kernel',\n",
       " 'asymmetry_coefficient',\n",
       " 'length_of_groove']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2ea9de32",
   "metadata": {},
   "outputs": [],
   "source": [
    "assembeler = VectorAssembler(inputCols= df.columns, outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "57f7828e",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = assembeler.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "157bee3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- area: double (nullable = true)\n",
      " |-- perimeter: double (nullable = true)\n",
      " |-- compactness: double (nullable = true)\n",
      " |-- length_of_kernel: double (nullable = true)\n",
      " |-- width_of_kernel: double (nullable = true)\n",
      " |-- asymmetry_coefficient: double (nullable = true)\n",
      " |-- length_of_groove: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c441151",
   "metadata": {},
   "source": [
    "### Scaling the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ee168aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a36618a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler(inputCol='features', outputCol='scaledFeatures')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "72876907",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "scaler_model = scaler.fit(final_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9ac8b05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = scaler_model.transform(final_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "672defa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(area=15.26, perimeter=14.84, compactness=0.871, length_of_kernel=5.763, width_of_kernel=3.312, asymmetry_coefficient=2.221, length_of_groove=5.22, features=DenseVector([15.26, 14.84, 0.871, 5.763, 3.312, 2.221, 5.22]), scaledFeatures=DenseVector([5.2445, 11.3633, 36.8608, 13.0072, 8.7685, 1.4772, 10.621]))]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad49294",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4948958b",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(featuresCol='scaledFeatures', k = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6a72d985",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "model = kmeans.fit(final_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5c183aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "wssse = model.summary.trainingCost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "137f0fbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "428.6082011872446"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wssse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3830961f",
   "metadata": {},
   "source": [
    "### This is not superuseful when you scale the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bf1eddbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "centers = model.clusterCenters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0bd6cd96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([ 6.35645488, 12.40730852, 37.41990178, 13.93860446,  9.7892399 ,\n",
      "        2.41585013, 12.29286107]), array([ 4.07497225, 10.14410142, 35.89816849, 11.80812742,  7.54416916,\n",
      "        3.15410901, 10.38031464]), array([ 4.96198582, 10.97871333, 37.30930808, 12.44647267,  8.62880781,\n",
      "        1.80061978, 10.41913733])]\n"
     ]
    }
   ],
   "source": [
    "print(centers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "33c1f3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.transform(final_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "84ff2971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|prediction|\n",
      "+----------+\n",
      "|         2|\n",
      "|         2|\n",
      "|         2|\n",
      "|         2|\n",
      "|         2|\n",
      "|         2|\n",
      "|         2|\n",
      "|         2|\n",
      "|         0|\n",
      "|         2|\n",
      "|         2|\n",
      "|         2|\n",
      "|         2|\n",
      "|         2|\n",
      "|         2|\n",
      "|         2|\n",
      "|         2|\n",
      "|         2|\n",
      "|         2|\n",
      "|         1|\n",
      "+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results.select('prediction').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a93367",
   "metadata": {},
   "source": [
    "# Practical Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3e594cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('hack_data.csv', header= True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3cce5d2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Session_Connection_Time: double (nullable = true)\n",
      " |-- Bytes Transferred: double (nullable = true)\n",
      " |-- Kali_Trace_Used: integer (nullable = true)\n",
      " |-- Servers_Corrupted: double (nullable = true)\n",
      " |-- Pages_Corrupted: double (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- WPM_Typing_Speed: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7efa75c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------------+------------------+------------------+-----------------+------------------+-----------+------------------+\n",
      "|summary|Session_Connection_Time| Bytes Transferred|   Kali_Trace_Used|Servers_Corrupted|   Pages_Corrupted|   Location|  WPM_Typing_Speed|\n",
      "+-------+-----------------------+------------------+------------------+-----------------+------------------+-----------+------------------+\n",
      "|  count|                    334|               334|               334|              334|               334|        334|               334|\n",
      "|   mean|     30.008982035928145| 607.2452694610777|0.5119760479041916|5.258502994011977|10.838323353293413|       null|57.342395209580864|\n",
      "| stddev|     14.088200614636158|286.33593163576757|0.5006065264451406| 2.30190693339697|  3.06352633036022|       null| 13.41106336843464|\n",
      "|    min|                    1.0|              10.0|                 0|              1.0|               6.0|Afghanistan|              40.0|\n",
      "|    max|                   60.0|            1330.5|                 1|             10.0|              15.0|   Zimbabwe|              75.0|\n",
      "+-------+-----------------------+------------------+------------------+-----------------+------------------+-----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c48556b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d1a711e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Session_Connection_Time',\n",
       " 'Bytes Transferred',\n",
       " 'Kali_Trace_Used',\n",
       " 'Servers_Corrupted',\n",
       " 'Pages_Corrupted',\n",
       " 'Location',\n",
       " 'WPM_Typing_Speed']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "544a04c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "assember = VectorAssembler(inputCols=['Session_Connection_Time',\n",
    " 'Bytes Transferred',\n",
    " 'Kali_Trace_Used',\n",
    " 'Servers_Corrupted',\n",
    " 'Pages_Corrupted','WPM_Typing_Speed'], outputCol= 'features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4df5b947",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = assember.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "dfa5d54f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Session_Connection_Time: double (nullable = true)\n",
      " |-- Bytes Transferred: double (nullable = true)\n",
      " |-- Kali_Trace_Used: integer (nullable = true)\n",
      " |-- Servers_Corrupted: double (nullable = true)\n",
      " |-- Pages_Corrupted: double (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- WPM_Typing_Speed: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "34ecc7f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Session_Connection_Time=8.0, Bytes Transferred=391.09, Kali_Trace_Used=1, Servers_Corrupted=2.96, Pages_Corrupted=7.0, Location='Slovenia', WPM_Typing_Speed=72.37, features=DenseVector([8.0, 391.09, 1.0, 2.96, 7.0, 72.37]))]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8b1daf",
   "metadata": {},
   "source": [
    "#### Scale the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1c941c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "15e4699b",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler(inputCol='features', outputCol='ScaledFeatures')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a6a9cc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_features = scaler.fit(final_data).transform(final_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7d1935f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Session_Connection_Time=8.0, Bytes Transferred=391.09, Kali_Trace_Used=1, Servers_Corrupted=2.96, Pages_Corrupted=7.0, Location='Slovenia', WPM_Typing_Speed=72.37, features=DenseVector([8.0, 391.09, 1.0, 2.96, 7.0, 72.37]), ScaledFeatures=DenseVector([0.5679, 1.3658, 1.9976, 1.2859, 2.2849, 5.3963]))]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_features.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "aa891510",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b11219a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(featuresCol='ScaledFeatures', k= 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "54db2fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = kmeans.fit(scaled_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "467ea4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "wssse = model.summary.trainingCost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "ea12d6be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "601.7707512676691"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wssse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "b1dfdb54",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.transform(scaled_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c267a2cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|prediction|\n",
      "+----------+\n",
      "|         1|\n",
      "|         1|\n",
      "|         1|\n",
      "|         1|\n",
      "|         1|\n",
      "|         1|\n",
      "|         1|\n",
      "|         1|\n",
      "|         1|\n",
      "|         1|\n",
      "|         1|\n",
      "|         1|\n",
      "|         1|\n",
      "|         1|\n",
      "|         1|\n",
      "|         1|\n",
      "|         1|\n",
      "|         1|\n",
      "|         1|\n",
      "|         1|\n",
      "+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results.select('prediction').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "b12bdb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import ClusteringEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "dda74e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = ClusteringEvaluator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "1095a75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "clust_eval = evaluator.evaluate(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "b5d6a2eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6683623593283755"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clust_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e46581",
   "metadata": {},
   "source": [
    "### With 3 k out clust_eval is 0.304, with k =2 its 0.668"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "ed4f80b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|prediction|count|\n",
      "+----------+-----+\n",
      "|         1|  167|\n",
      "|         0|  167|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results.groupBy('prediction').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2937c9",
   "metadata": {},
   "source": [
    "### We can see now that data is getting evenly split between two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca82ecb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
